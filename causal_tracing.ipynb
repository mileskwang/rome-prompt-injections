{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f8c1302ba30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, json\n",
    "import torch, numpy\n",
    "from collections import defaultdict\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import (\n",
    "    ModelAndTokenizer,\n",
    "    layername,\n",
    "    guess_subject,\n",
    "    # plot_trace_heatmap,\n",
    ")\n",
    "from experiments.causal_trace import (\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_token,\n",
    "    predict_from_input,\n",
    "    collect_embedding_std,\n",
    ")\n",
    "from dsets import KnownsDataset\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6485ea6cdc514fea8336c2e7e25f0d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 44.35 GiB total capacity; 18.17 GiB already allocated; 70.12 MiB free; 18.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# or \"gpt2-xl\" or \"EleutherAI/gpt-j-6B\" or \"EleutherAI/gpt-neox-20b\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mt \u001b[39m=\u001b[39m ModelAndTokenizer(\n\u001b[1;32m      3\u001b[0m     model_name,\n\u001b[1;32m      4\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Set to False since you're not using Google Colab\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49m(torch\u001b[39m.\u001b[39;49mfloat16 \u001b[39mif\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m20b\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m model_name \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/net/scratch/milesw/rome-prompt-injections/experiments/causal_trace.py:469\u001b[0m, in \u001b[0;36mModelAndTokenizer.__init__\u001b[0;34m(self, model_name, model, tokenizer, low_cpu_mem_usage, torch_dtype)\u001b[0m\n\u001b[1;32m    465\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    466\u001b[0m         model_name, low_cpu_mem_usage\u001b[39m=\u001b[39mlow_cpu_mem_usage, torch_dtype\u001b[39m=\u001b[39mtorch_dtype\n\u001b[1;32m    467\u001b[0m     )\n\u001b[1;32m    468\u001b[0m     nethook\u001b[39m.\u001b[39mset_requires_grad(\u001b[39mFalse\u001b[39;00m, model)\n\u001b[0;32m--> 469\u001b[0m     model\u001b[39m.\u001b[39;49meval()\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m    470\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/net/scratch/milesw/miniconda3/envs/vicuna/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 44.35 GiB total capacity; 18.17 GiB already allocated; 70.12 MiB free; 18.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # or \"gpt2-xl\" or \"EleutherAI/gpt-j-6B\" or \"EleutherAI/gpt-neox-20b\"\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,  # Set to False since you're not using Google Colab\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use cuda if available\n",
    "mt.model = mt.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in mt.model.named_modules():\n",
    "#     print(name)\n",
    "# layer_names = [f\"model.layers.{layer_num}\" for layer_num in range(num_layers)]\n",
    "# print(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inputs(tokenizer, prompts, device=\"cuda\"):\n",
    "    token_lists = [tokenizer.encode(p) for p in prompts]\n",
    "    maxlen = max(len(t) for t in token_lists)\n",
    "    if \"[PAD]\" in tokenizer.all_special_tokens:\n",
    "        pad_id = tokenizer.all_special_ids[tokenizer.all_special_tokens.index(\"[PAD]\")]\n",
    "    else:\n",
    "        pad_id = 0\n",
    "    input_ids = [[pad_id] * (maxlen - len(t)) + t for t in token_lists]\n",
    "    # position_ids = [[0] * (maxlen - len(t)) + list(range(len(t))) for t in token_lists]\n",
    "    attention_mask = [[0] * (maxlen - len(t)) + [1] * len(t) for t in token_lists]\n",
    "    return dict(\n",
    "        input_ids=torch.tensor(input_ids).to(device),\n",
    "        #    position_ids=torch.tensor(position_ids).to(device),\n",
    "        attention_mask=torch.tensor(attention_mask).to(device),\n",
    "    )\n",
    "\n",
    "def predict_token(mt, prompts, return_p=False, temp: float = 0):\n",
    "    inp = make_inputs(mt.tokenizer, prompts)\n",
    "    preds, p = predict_from_input(mt.model, inp, temp=temp)\n",
    "    result = [mt.tokenizer.decode(c) for c in preds]\n",
    "    if return_p:\n",
    "        result = (result, p)\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict_from_input(model, inp, temp: float = 0):\n",
    "    out = model(**inp)[\"logits\"]\n",
    "    probs = torch.softmax(out[:, -1], dim=1)\n",
    "    \n",
    "    if temp == 0:\n",
    "        p, preds = torch.max(probs, dim=1)\n",
    "    else:\n",
    "        preds = torch.multinomial(probs, num_samples=1)\n",
    "        p = probs[torch.arange(probs.shape[0]), preds[:, 0]]\n",
    "\n",
    "    return preds, p\n",
    "\n",
    "\n",
    "def predict_multiple_tokens(\n",
    "    mt,\n",
    "    prompts: list[str],\n",
    "    n_tokens: int = 3,\n",
    "):\n",
    "    results, ps = [], []\n",
    "    for _ in range(n_tokens):\n",
    "        result, p = predict_token(mt, prompts, return_p=True)\n",
    "        results.append(result)\n",
    "        ps.append(p.item())\n",
    "\n",
    "        prompts = [p + r for p, r in zip(prompts, result)]\n",
    "\n",
    "    return results, ps\n",
    "\n",
    "def print_multiple_tokens(mt, prompt, n_tokens=3, temp: float = 0):\n",
    "    for i in range(n_tokens):\n",
    "        result, = predict_token(mt, [prompt], return_p=False, temp=temp)\n",
    "        print(result, end=\"\")\n",
    "        if (i + 1) % 30 == 0:\n",
    "            print()\n",
    "        prompt += result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp(tokens, device=\"cuda\"):\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    attention_mask = torch.tensor([[1] * len(tokens)]).to(device)\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "def get_prediction(model, tokenizer, tokens):\n",
    "    input = make_inp(tokens)\n",
    "    logits = model(**input)[\"logits\"]\n",
    "    probs = torch.softmax(logits[:, -1], dim=1)\n",
    "    max_prob, preds = torch.max(probs, dim=1)\n",
    "    result = [tokenizer.decode(c) for c in preds]\n",
    "    return result, max_prob.item()\n",
    "\n",
    "def get_next_k_tokens(model, tokenizer, tokens, k=5):\n",
    "    result_tokens = []\n",
    "    result_probs = []\n",
    "    tokens = list(tokens)  # Create a copy of tokens to prevent modifying the original list\n",
    "    for _ in range(k):\n",
    "        input = make_inp(tokens)\n",
    "        logits = model(**input)[\"logits\"]\n",
    "        probs = torch.softmax(logits[:, -1], dim=1)\n",
    "        max_prob, preds = torch.max(probs, dim=1)\n",
    "        next_token = preds.item()  # Convert to Python int\n",
    "        result_tokens.append(next_token)\n",
    "        tokens.append(next_token)  # Update the tokens with the new prediction\n",
    "        result_probs.append(max_prob.item())\n",
    "    # Convert token IDs back to tokens for the final output\n",
    "    result_tokens = [tokenizer.decode([t]) for t in result_tokens]\n",
    "    return result_tokens, result_probs\n",
    "\n",
    "def get_top_k_predictions(model, tokenizer, tokens, k):\n",
    "    input = make_inp(tokens)\n",
    "    logits = model(**input)[\"logits\"]\n",
    "    probs = torch.softmax(logits[:, -1], dim=1)\n",
    "    topk_probs, topk_inds = torch.topk(probs, k)\n",
    "    topk_results = [tokenizer.decode(c) for c in topk_inds[0]]\n",
    "    topk_probs = topk_probs[0].cpu().numpy().tolist()\n",
    "    return topk_results, topk_probs\n",
    "\n",
    "def get_token_probabilities(model, tokenizer, tokens, token_list, k=2):\n",
    "    input = make_inp(tokens)\n",
    "    # prompt = tokenizer.decode(tokens)\n",
    "    # input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    logits = model(**input)[\"logits\"]\n",
    "    probs = torch.softmax(logits[:, -1], dim=1)\n",
    "    topk_probs, topk_inds = torch.topk(probs, 5) ##topk\n",
    "    topk_results = [tokenizer.decode(c) for c in topk_inds[0]] ##topk\n",
    "    topk_probs = topk_probs[0].cpu().numpy().tolist() ##topk\n",
    "    topk = list(zip(topk_results, topk_probs)) ##topk\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(token_list)\n",
    "    token_probs = probs[0, token_ids]\n",
    "    token_probs = token_probs.tolist()\n",
    "    token_list = [tokenizer.decode([id]) for id in token_ids]\n",
    "    results, _ = get_next_k_tokens(model, tokenizer, tokens, k=k)\n",
    "    result = \"\".join(results[:k])\n",
    "    return token_list, token_probs, result, topk\n",
    "\n",
    "## thoughts\n",
    "# For each ablation, get the most probable next token for each state that is not significantly paris\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowns = KnownsDataset(DATA_DIR)  # Dataset of known facts\n",
    "noise_level = 3 * collect_embedding_std(mt, [k[\"subject\"] for k in knowns])\n",
    "print(f\"Using noise level {noise_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/net/scratch/milesw/gpt2xl_outputs_14\"  # or \"gpt2-xl\" or \"EleutherAI/gpt-j-6B\" or \"EleutherAI/gpt-neox-20b\"\n",
    "mt2 = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,  # Set to False since you're not using Google Colab\n",
    "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = \"The Space Needle is in the city of\"\n",
    "fact = \"The Taj Mahal is in the country of\"\n",
    "fact = \"The Great Wall of China is in the country of\"\n",
    "fact = \"The Great Pyramid of Giza is in the country of\"\n",
    "fact = \"Mount Fuji is in the country of\"\n",
    "fact = \"The Leaning Tower of Pisa is in the city of\"\n",
    "fact = \"The Sydney Opera House is in the city of\"\n",
    "fact = \"Machu Picchu is in the country of\"\n",
    "fact = \"The Eiffel Tower is in the city of\"\n",
    "fact = \"The Colosseum is in the city of\"\n",
    "\n",
    "facts = [\n",
    "    \"The Taj Mahal is in the country of\",\n",
    "    \"The Great Wall of China is in the country of\",\n",
    "    \"The Great Pyramid of Giza is in the country of\",\n",
    "    \"Mount Fuji is in the country of\",\n",
    "    \"Machu Picchu is in the country of\",\n",
    "    \"The Amazon rainforest is in the country of\",\n",
    "    \"The Acropolis is in the city of\",\n",
    "    \"The Leaning Tower of Pisa is in the city of\",\n",
    "    \"The Sydney Opera House is in the city of\",\n",
    "    \"The Eiffel Tower is in the city of\",\n",
    "    \"The Colosseum is in the city of\"\n",
    "]\n",
    "# Turn all the facts into a list of prompts\n",
    "\n",
    "incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are an assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "correct_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are an assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a correct answer. {fact} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fact in facts:\n",
    "    print(fact)\n",
    "    incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Always respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "    correct_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Always respond with a correct answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "    print(predict_multiple_tokens(\n",
    "    mt,\n",
    "        [incorrect_prompt],\n",
    "        n_tokens=3,\n",
    "    ))\n",
    "\n",
    "    print(get_top_k_predictions(mt.model, mt.tokenizer, mt.tokenizer.encode(incorrect_prompt), 7))\n",
    "                          \n",
    "    print(predict_multiple_tokens(\n",
    "        mt,\n",
    "        [correct_prompt],\n",
    "        n_tokens=3,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = \"The Eiffel Tower is in the city of\"\n",
    "incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "encoded = mt.tokenizer.encode(incorrect_prompt)[29:]\n",
    "print(mt.tokenizer.decode(encoded))\n",
    "\n",
    "correct_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a correct answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "encoded = mt.tokenizer.encode(correct_prompt)[29:]\n",
    "print(mt.tokenizer.decode(encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_multiple_tokens(\n",
    "    mt,\n",
    "    [\"Megan Rapinoe plays the sport of\"],\n",
    "    n_tokens=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(\n",
    "    mt,\n",
    "    prompt: str,\n",
    "    words_to_check: dict[str],\n",
    "    n_tokens: int = 20,\n",
    "    n_iters: int = 10,\n",
    "    temp: int = 1.0,\n",
    ") -> dict[str, int]:\n",
    "    counter = {word: 0 for word in words_to_check}\n",
    "    for _ in range(n_iters):\n",
    "        results, _ = predict_multiple_tokens(mt, [prompt], n_tokens=n_tokens, temp=temp)\n",
    "        results_flat = [\n",
    "            item for sublist in results for item in sublist\n",
    "        ]  # Flatten the list\n",
    "        for result in results_flat:\n",
    "            for word in words_to_check:\n",
    "                if word in result:\n",
    "                    counter[word] += 1\n",
    "    for word, count in counter.items():\n",
    "        print(\n",
    "            f\"The word '{word}' appeared {count} times in {n_iters} predictions of {n_tokens} tokens each.\"\n",
    "        )\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_with_patch(\n",
    "    model,  # The model\n",
    "    inp,  # A set of inputs\n",
    "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
    "    answers_t,  # Answer probabilities to collect\n",
    "    tokens_to_mix,  # Range of tokens to corrupt (begin, end)\n",
    "    noise=0.1,  # Level of noise to add\n",
    "    trace_layers=None,  # List of traced outputs to return\n",
    "):\n",
    "    prng = numpy.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
    "    patch_spec = defaultdict(list)\n",
    "    for t, l in states_to_patch:\n",
    "        patch_spec[l].append(t)\n",
    "    embed_layername = layername(model, 0, \"embed\")\n",
    "\n",
    "    def untuple(x): # for layers that output tuples (such as hidden states and attention weights, return first element) TODO: which element is this\n",
    "        return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    # Define the model-patching rule.\n",
    "    def patch_rep(x, layer):\n",
    "        if layer == embed_layername:\n",
    "            # If requested, we corrupt a range of token embeddings on batch items x[1:]\n",
    "            if tokens_to_mix is not None:\n",
    "                b, e = tokens_to_mix\n",
    "                x[1:, b:e] += noise * torch.from_numpy(\n",
    "                    prng.randn(x.shape[0] - 1, e - b, x.shape[2])\n",
    "                ).to(x.device)\n",
    "            return x\n",
    "        if layer not in patch_spec:\n",
    "            return x\n",
    "        # If this layer is in the patch_spec, restore the uncorrupted hidden state\n",
    "        # for selected tokens.\n",
    "        h = untuple(x)\n",
    "        for t in patch_spec[layer]:\n",
    "            h[1:, t] = h[0, t]\n",
    "        return x\n",
    "\n",
    "    # With the patching rules defined, run the patched model in inference.\n",
    "    additional_layers = [] if trace_layers is None else trace_layers\n",
    "    with torch.no_grad(), nethook.TraceDict(\n",
    "        model,\n",
    "        [embed_layername] + list(patch_spec.keys()) + additional_layers,\n",
    "        edit_output=patch_rep,\n",
    "    ) as td:\n",
    "        outputs_exp = model(**inp)\n",
    "\n",
    "    # We report softmax probabilities for the answers_t token predictions of interest.\n",
    "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
    "\n",
    "    # If tracing all layers, collect all activations together to return.\n",
    "    if trace_layers is not None:\n",
    "        all_traced = torch.stack(\n",
    "            [untuple(td[layer].output).detach().cpu() for layer in trace_layers], dim=2\n",
    "        )\n",
    "        return probs, all_traced\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hidden_flow(\n",
    "    mt, prompt, subject, samples=10, noise=0.1, window=10, kind=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs causal tracing over every token/layer combination in the network\n",
    "    and returns a dictionary numerically summarizing the results.\n",
    "    \"\"\"\n",
    "    inp = make_inputs(mt.tokenizer, [prompt] * (samples + 1))\n",
    "    with torch.no_grad():\n",
    "        answer_t, base_score = [d[0] for d in predict_from_input(mt.model, inp)]\n",
    "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "    e_range = find_token_range(mt.tokenizer, inp[\"input_ids\"][0], subject)\n",
    "    low_score = trace_with_patch(\n",
    "        mt.model, inp, [], answer_t, e_range, noise=noise\n",
    "    ).item()\n",
    "    if not kind:\n",
    "        differences = trace_important_states(\n",
    "            mt.model, mt.num_layers, inp, e_range, answer_t, noise=noise\n",
    "        )\n",
    "    else:\n",
    "        differences = trace_important_window(\n",
    "            mt.model,\n",
    "            mt.num_layers,\n",
    "            inp,\n",
    "            e_range,\n",
    "            answer_t,\n",
    "            noise=noise,\n",
    "            window=window,\n",
    "            kind=kind,\n",
    "        )\n",
    "    differences = differences.detach().cpu()\n",
    "    input_i = inp[\"input_ids\"][0]\n",
    "    input_ids = input_i[29:]\n",
    "    input_tokens = decode_tokens(mt.tokenizer, input_ids)\n",
    "\n",
    "    return dict(\n",
    "        scores=differences,\n",
    "        low_score=low_score,\n",
    "        high_score=base_score,\n",
    "        input_ids=input_ids,\n",
    "        input_tokens=input_tokens,\n",
    "        # input_ids=inp[\"input_ids\"][0],\n",
    "        # input_tokens=decode_tokens(mt.tokenizer, inp[\"input_ids\"][0]),\n",
    "        subject_range=e_range,\n",
    "        answer=answer,\n",
    "        window=window,\n",
    "        kind=kind or \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def trace_important_states(model, num_layers, inp, e_range, answer_t, noise=0.1):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    # for tnum in range(ntoks):\n",
    "    for tnum in range(29, ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            r = trace_with_patch(\n",
    "                model,\n",
    "                inp,\n",
    "                [(tnum, layername(model, layer))],\n",
    "                answer_t,\n",
    "                tokens_to_mix=e_range,\n",
    "                noise=noise,\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)\n",
    "\n",
    "\n",
    "def trace_important_window(\n",
    "    model, num_layers, inp, e_range, answer_t, kind, window=7, noise=0.1\n",
    "):\n",
    "    ntoks = inp[\"input_ids\"].shape[1]\n",
    "    table = []\n",
    "    \n",
    "    # for tnum in range(ntoks):\n",
    "    for tnum in range(29, ntoks):\n",
    "        row = []\n",
    "        for layer in range(0, num_layers):\n",
    "            layerlist = [\n",
    "                (tnum, layername(model, L, kind))\n",
    "                for L in range(\n",
    "                    max(0, layer - window // 2), min(num_layers, layer - (-window // 2))\n",
    "                )\n",
    "            ]\n",
    "            r = trace_with_patch(\n",
    "                model, inp, layerlist, answer_t, tokens_to_mix=e_range, noise=noise\n",
    "            )\n",
    "            row.append(r)\n",
    "        table.append(torch.stack(row))\n",
    "    return torch.stack(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_trace_heatmap(result, savepdf=None, title=None, xlabel=None, modelname=None):\n",
    "    differences = result[\"scores\"]\n",
    "    low_score = result[\"low_score\"]\n",
    "    answer = result[\"answer\"]\n",
    "    kind = (\n",
    "        None\n",
    "        if (not result[\"kind\"] or result[\"kind\"] == \"None\")\n",
    "        else str(result[\"kind\"])\n",
    "    )\n",
    "    window = result.get(\"window\", 7)\n",
    "    labels = list(result[\"input_tokens\"])\n",
    "    # for i in range(*result[\"subject_range\"]): ##remove for doing correct vs incorrect\n",
    "    #     labels[i] = labels[i] + \"*\"\n",
    "\n",
    "    with plt.rc_context():\n",
    "        fig, ax = plt.subplots(figsize=(3.5, 2), dpi=200)\n",
    "        h = ax.pcolor(\n",
    "            differences,\n",
    "            cmap={None: \"Purples\", \"None\": \"Purples\", \"mlp\": \"Greens\", \"attn\": \"Reds\"}[\n",
    "                kind\n",
    "            ],\n",
    "            vmin=low_score,\n",
    "        )\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_yticks([0.5 + i for i in range(len(differences))])\n",
    "        ax.set_xticks([0.5 + i for i in range(0, differences.shape[1] - 6, 5)])\n",
    "        ax.set_xticklabels(list(range(0, differences.shape[1] - 6, 5)))\n",
    "        ax.set_yticklabels(labels, fontsize=5)\n",
    "        if not modelname:\n",
    "            modelname = \"Llama-2-7b\"\n",
    "        if not kind:\n",
    "            ax.set_title(\"Impact of restoring state after corrupted input\")\n",
    "            ax.set_xlabel(f\"single restored layer within {modelname}\")\n",
    "        else:\n",
    "            kindname = \"MLP\" if kind == \"mlp\" else \"Attn\"\n",
    "            ax.set_title(f\"Impact of restoring {kindname} after corrupted input\")\n",
    "            ax.set_xlabel(f\"center of interval of 7 restored {kindname} layers\")\n",
    "        cb = plt.colorbar(h)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        if xlabel is not None:\n",
    "            ax.set_xlabel(xlabel)\n",
    "        elif answer is not None:\n",
    "            # The following should be cb.ax.set_xlabel, but this is broken in matplotlib 3.5.1.\n",
    "            cb.ax.set_title(f\"p({str(answer).strip()})\", y=-0.16, fontsize=10)\n",
    "        if savepdf:\n",
    "            os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n",
    "            plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hidden_flow(\n",
    "    mt,\n",
    "    prompt,\n",
    "    subject=None,\n",
    "    samples=10,\n",
    "    noise=0.1,\n",
    "    window=10,\n",
    "    kind=None,\n",
    "    modelname=None,\n",
    "    savepdf=None,\n",
    "):\n",
    "    if subject is None:\n",
    "        subject = guess_subject(prompt)\n",
    "    result = calculate_hidden_flow(\n",
    "        mt, prompt, subject, samples=samples, noise=noise, window=window, kind=kind\n",
    "    )\n",
    "    plot_trace_heatmap(result, savepdf, modelname=modelname)\n",
    "\n",
    "\n",
    "def plot_all_flow(mt, prompt, subject=None, noise=0.1, modelname=None):\n",
    "    for kind in [None, \"mlp\", \"attn\"]:\n",
    "        plot_hidden_flow(\n",
    "            mt, prompt, subject, modelname=modelname, noise=noise, kind=kind\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = \"The Eiffel Tower is in the city of\"\n",
    "incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "encoded = mt.tokenizer.encode(incorrect_prompt)[29:]\n",
    "print(mt.tokenizer.decode(encoded))\n",
    "\n",
    "correct_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a correct answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "fact = \"The Eiffel Tower is in the city of\"\n",
    "\n",
    "\n",
    "incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "encoded = mt.tokenizer.encode(correct_prompt)[29:]\n",
    "print(mt.tokenizer.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = \"The Eiffel Tower is in the city of\"\n",
    "incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Always respond with an incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "print(incorrect_prompt)\n",
    "\n",
    "print(predict_multiple_tokens(\n",
    "    mt,\n",
    "    [incorrect_prompt],\n",
    "    n_tokens=3,\n",
    "    ))\n",
    "\n",
    "facts = [\"The Eiffel Tower is in the city of\"]\n",
    "\n",
    "for fact in facts:\n",
    "    incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "    print(incorrect_prompt)\n",
    "    print(predict_multiple_tokens(\n",
    "    mt,\n",
    "        [incorrect_prompt],\n",
    "        n_tokens=3,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = \"The Sydney Opera House is in the city of\"\n",
    "incorrect_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "encoded = mt.tokenizer.encode(incorrect_prompt)[29:]\n",
    "print(mt.tokenizer.decode(encoded))\n",
    "print(predict_multiple_tokens(\n",
    "mt,\n",
    "    [incorrect_prompt],\n",
    "    n_tokens=3,\n",
    "))\n",
    "\n",
    "correct_prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant that responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a correct answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "encoded = mt.tokenizer.encode(correct_prompt)[29:]\n",
    "print(mt.tokenizer.decode(encoded))\n",
    "print(predict_multiple_tokens(\n",
    "    mt,\n",
    "    [correct_prompt],\n",
    "    n_tokens=3,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_flow(mt, incorrect_prompt, subject=\"SydneyOperaHouse\", noise=noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_flow(mt, correct_prompt, subject=\"SydneyOperaHouse\", noise=noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\\\n",
    "<s> [INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "The Eiffel Tower is in the city of [/INST]\n",
    "\"\"\"\n",
    "\n",
    "plot_all_flow(mt, prompt, subject=\"EiffelTower\", noise=noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Space Needle is in the city of\"\n",
    "inp = make_inputs(mt.tokenizer, [prompt] * (10 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_range = find_token_range(mt.tokenizer, inp[\"input_ids\"][0], \"Space Needle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inp[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = decode_tokens(mt.tokenizer, inp[\"input_ids\"][0])\n",
    "print(toks)\n",
    "mt.tokenizer.decode(inp[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_string = \"\".join(toks)\n",
    "print(whole_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_loc = whole_string.index(\"Space Needle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    toks = decode_tokens(tokenizer, token_array)\n",
    "    whole_string = \"\".join(toks)\n",
    "    char_loc = whole_string.index(substring)\n",
    "    loc = 0\n",
    "    tok_start, tok_end = None, None\n",
    "    for i, t in enumerate(toks):\n",
    "        loc += len(t)\n",
    "        if tok_start is None and loc > char_loc:\n",
    "            tok_start = i\n",
    "        if tok_end is None and loc >= char_loc + len(substring):\n",
    "            tok_end = i + 1\n",
    "            break\n",
    "    return (tok_start, tok_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Space Needle is in the city of\"\n",
    "inp = make_inputs(mt2.tokenizer, [prompt] * (10 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inp[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = decode_tokens(mt2.tokenizer, inp[\"input_ids\"][0])\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_string = \"\".join(toks)\n",
    "print(whole_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
