{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class AttnWrapper(torch.nn.Module):\n",
    "    def __init__(self, attn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.activations = None\n",
    "        self.add_tensor = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.attn(*args, **kwargs)\n",
    "        if self.add_tensor is not None:\n",
    "            output = (output[0] + self.add_tensor,)+output[1:]\n",
    "        self.activations = output[0]\n",
    "        return output\n",
    "\n",
    "    def reset(self):\n",
    "        self.activations = None\n",
    "        self.add_tensor = None\n",
    "\n",
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "\n",
    "        self.block.self_attn = AttnWrapper(self.block.self_attn)\n",
    "        self.post_attention_layernorm = self.block.post_attention_layernorm\n",
    "\n",
    "        self.attn_mech_output_unembedded = None\n",
    "        self.intermediate_res_unembedded = None\n",
    "        self.mlp_output_unembedded = None\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.block_output = self.norm(output[0])\n",
    "        attn_output = self.block.self_attn.activations\n",
    "        self.attn_mech_output = self.norm(attn_output)\n",
    "        attn_output += args[0]\n",
    "        self.intermediate_res = self.norm(attn_output)\n",
    "        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n",
    "        self.mlp_output_unembedded = self.norm(mlp_output)\n",
    "        return output\n",
    "\n",
    "    def attn_add_tensor(self, tensor):\n",
    "        self.block.self_attn.add_tensor = tensor\n",
    "\n",
    "    def reset(self):\n",
    "        self.block.self_attn.reset()\n",
    "\n",
    "    def get_attn_activations(self):\n",
    "        return self.block.self_attn.activations\n",
    "\n",
    "class Llama7BHelper:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\").to(self.device)\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            self.model.model.layers[i] = BlockOutputWrapper(layer, self.model.lm_head, self.model.model.norm)\n",
    "\n",
    "    def generate_text(self, prompt, max_length=100):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generate_ids = self.model.generate(inputs.input_ids.to(self.device), max_length=max_length)\n",
    "        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    def get_logits(self, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "          logits = self.model(inputs.input_ids.to(self.device)).logits\n",
    "          return logits\n",
    "\n",
    "    def set_add_attn_output(self, layer, add_output):\n",
    "        self.model.model.layers[layer].attn_add_tensor(add_output)\n",
    "\n",
    "    def get_attn_activations(self, layer):\n",
    "        return self.model.model.layers[layer].get_attn_activations()\n",
    "\n",
    "    def reset_all(self):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.reset()\n",
    "\n",
    "    def print_decoded_activations(self, decoded_activations, label):\n",
    "        softmaxed = torch.nn.functional.softmax(decoded_activations[0][-1], dim=-1)\n",
    "        values, indices = torch.topk(softmaxed, 10)\n",
    "        probs_percent = [int(v * 100) for v in values.tolist()]\n",
    "        tokens = self.tokenizer.batch_decode(indices.unsqueeze(-1))\n",
    "        print(label, list(zip(tokens, probs_percent)))\n",
    "\n",
    "\n",
    "    def decode_all_layers(self, text, topk=10, print_attn_mech=True, print_intermediate_res=True, print_mlp=True, print_block=True):\n",
    "        self.get_logits(text)\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            print(f'L{i}:')\n",
    "            if print_attn_mech:\n",
    "                self.print_decoded_activations(layer.attn_mech_output_unembedded, 'Attn')\n",
    "            if print_intermediate_res:\n",
    "                self.print_decoded_activations(layer.intermediate_res_unembedded, 'Resid')\n",
    "            if print_mlp:\n",
    "                self.print_decoded_activations(layer.mlp_output_unembedded, 'MLP')\n",
    "            if print_block:\n",
    "                self.print_decoded_activations(layer.block_output_unembedded, 'Blck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, tokenizer, tokens):\n",
    "    input = make_inp(tokens)\n",
    "    logits = model(**input)[\"logits\"]\n",
    "    probs = torch.softmax(logits[:, -1], dim=1)\n",
    "    max_prob, preds = torch.max(probs, dim=1)\n",
    "    result = [tokenizer.decode(c) for c in preds]\n",
    "    return result, max_prob.item()\n",
    "\n",
    "def get_next_k_tokens(model, tokenizer, tokens, k=5):\n",
    "    result_tokens = []\n",
    "    result_probs = []\n",
    "    tokens = list(tokens)  # Create a copy of tokens to prevent modifying the original list\n",
    "    for _ in range(k):\n",
    "        input = make_inp(tokens)\n",
    "        logits = model(**input)[\"logits\"]\n",
    "        probs = torch.softmax(logits[:, -1], dim=1)\n",
    "        max_prob, preds = torch.max(probs, dim=1)\n",
    "        next_token = preds.item()  # Convert to Python int\n",
    "        result_tokens.append(next_token)\n",
    "        tokens.append(next_token)  # Update the tokens with the new prediction\n",
    "        result_probs.append(max_prob.item())\n",
    "    # Convert token IDs back to tokens for the final output\n",
    "    result_tokens = [tokenizer.decode([t]) for t in result_tokens]\n",
    "    return result_tokens, result_probs\n",
    "\n",
    "def get_top_k_predictions(model, tokenizer, tokens, k):\n",
    "    input = make_inp(tokens)\n",
    "    logits = model(**input)[\"logits\"]\n",
    "    probs = torch.softmax(logits[:, -1], dim=1)\n",
    "    topk_probs, topk_inds = torch.topk(probs, k)\n",
    "    topk_results = [tokenizer.decode(c) for c in topk_inds[0]]\n",
    "    topk_probs = topk_probs[0].cpu().numpy().tolist()\n",
    "    return topk_results, topk_probs\n",
    "\n",
    "def get_token_probabilities(model, tokenizer, tokens, token_list, k=2):\n",
    "    input = make_inp(tokens)\n",
    "    # prompt = tokenizer.decode(tokens)\n",
    "    # input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    logits = model(**input)[\"logits\"]\n",
    "    probs = torch.softmax(logits[:, -1], dim=1)\n",
    "    topk_probs, topk_inds = torch.topk(probs, 5) ##topk\n",
    "    topk_results = [tokenizer.decode(c) for c in topk_inds[0]] ##topk\n",
    "    topk_probs = topk_probs[0].cpu().numpy().tolist() ##topk\n",
    "    topk = list(zip(topk_results, topk_probs)) ##topk\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(token_list)\n",
    "    token_probs = probs[0, token_ids]\n",
    "    token_probs = token_probs.tolist()\n",
    "    token_list = [tokenizer.decode([id]) for id in token_ids]\n",
    "    results, _ = get_next_k_tokens(model, tokenizer, tokens, k=k)\n",
    "    result = \"\".join(results[:k])\n",
    "    return token_list, token_probs, result, topk\n",
    "\n",
    "def make_inp(tokens, device=\"cuda\"):\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    attention_mask = torch.tensor([[1] * len(tokens)]).to(device)\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7895527a1054aedb73defab5bdcb9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Llama7BHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "facts = [\n",
    "    \"The Mount Rushmore is in the country of\",\n",
    "    \"The Taj Mahal is in the country of\",\n",
    "    \"The Great Wall of China is in the country of\",\n",
    "    \"Machu Picchu is in the country of\",\n",
    "    \"The Amazon rainforest is in the country of\",\n",
    "    \"The Eiffel Tower is in the city of\",\n",
    "    \"The Colosseum is in the city of\",\n",
    "    \"The Yellowstone Park is in the country of\",\n",
    "    \"The Christ Redeemer is in the city of\",\n",
    "    \"\\The Angkor Wat is in the country of\",\n",
    "    \"\\The Buckingham Palace is in the city of\",\n",
    "    \"\\The Victoria Falls is in the country of\",\n",
    "    \"The Sahara Desert is in the country of\",\n",
    "    \"\\nThe Sydney Opera is in the city of\",\n",
    "    \"\\nThe White House is in the city of\",\n",
    "    \"\\The Stonehenge is in the country of\",\n",
    "    \"The Chichen Itza is in the country of\",\n",
    "    \"\\The Mount Fuji is in the country of\",\n",
    "]\n",
    "\n",
    "for fact in facts:\n",
    "    print(len(model.tokenizer.encode(fact)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# facts = [\n",
    "#     \"The Taj Mahal is in the country of\",\n",
    "#     \"The Great Wall of China is in the country of\",\n",
    "#     \"The Great Pyramid of Giza is in the country of\",\n",
    "#     \"Mount Fuji is in the country of\",\n",
    "#     \"Machu Picchu is in the country of\",\n",
    "#     \"The Amazon rainforest is in the country of\",\n",
    "#     \"The Acropolis is in the city of\",\n",
    "#     \"The Leaning Tower of Pisa is in the city of\",\n",
    "#     \"The Sydney Opera House is in the city of\",\n",
    "#     \"The Eiffel Tower is in the city of\",\n",
    "#     \"The Colosseum is in the city of\"\n",
    "# ]\n",
    "\n",
    "prompt_pairs = []\n",
    "print(len(facts))\n",
    "\n",
    "for fact in facts:\n",
    "    correct = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a correct answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "    incorrect = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. {fact} [/INST]\n",
    "\"\"\"\n",
    "    prompt_pairs.append((correct, incorrect))\n",
    "\n",
    "print(len(prompt_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(prompt_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Sahara Desert is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Sahara Desert is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Great Wall of China is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Great Wall of China is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Eiffel Tower is in the city of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Eiffel Tower is in the city of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Mount Rushmore is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Mount Rushmore is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. \\nThe White House is in the city of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. \\nThe White House is in the city of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. \\\\The Stonehenge is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. \\\\The Stonehenge is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. \\\\The Buckingham Palace is in the city of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. \\\\The Buckingham Palace is in the city of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. \\\\The Victoria Falls is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. \\\\The Victoria Falls is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. \\\\The Mount Fuji is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. \\\\The Mount Fuji is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Colosseum is in the city of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Colosseum is in the city of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Taj Mahal is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Taj Mahal is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. \\nThe Sydney Opera is in the city of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. \\nThe Sydney Opera is in the city of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Chichen Itza is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Chichen Itza is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Christ Redeemer is in the city of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Christ Redeemer is in the city of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Yellowstone Park is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Yellowstone Park is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. Machu Picchu is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. Machu Picchu is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. The Amazon rainforest is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. The Amazon rainforest is in the country of [/INST]\\n')\n",
      "('[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a correct answer. \\\\The Angkor Wat is in the country of [/INST]\\n', '[INST] <<SYS>>\\nYou are a helpful assistant who responds with one word.\\n<</SYS>>\\n\\nRespond with a incorrect answer. \\\\The Angkor Wat is in the country of [/INST]\\n')\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_pairs, val_pairs = train_test_split(prompt_pairs, test_size=0.5)\n",
    "\n",
    "for pair in train_pairs:\n",
    "    print(pair)\n",
    "for pair in val_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Accuracy: 1.0\n",
      "Layer 1 Accuracy: 1.0\n",
      "Layer 2 Accuracy: 1.0\n",
      "Layer 3 Accuracy: 1.0\n",
      "Layer 4 Accuracy: 1.0\n",
      "Layer 5 Accuracy: 1.0\n",
      "Layer 6 Accuracy: 1.0\n",
      "Layer 7 Accuracy: 1.0\n",
      "Layer 8 Accuracy: 1.0\n",
      "Layer 9 Accuracy: 1.0\n",
      "Layer 10 Accuracy: 1.0\n",
      "Layer 11 Accuracy: 1.0\n",
      "Layer 12 Accuracy: 1.0\n",
      "Layer 13 Accuracy: 1.0\n",
      "Layer 14 Accuracy: 1.0\n",
      "Layer 15 Accuracy: 1.0\n",
      "Layer 16 Accuracy: 1.0\n",
      "Layer 17 Accuracy: 1.0\n",
      "Layer 18 Accuracy: 1.0\n",
      "Layer 19 Accuracy: 1.0\n",
      "Layer 20 Accuracy: 1.0\n",
      "Layer 21 Accuracy: 1.0\n",
      "Layer 22 Accuracy: 1.0\n",
      "Layer 23 Accuracy: 1.0\n",
      "Layer 24 Accuracy: 1.0\n",
      "Layer 25 Accuracy: 1.0\n",
      "Layer 26 Accuracy: 1.0\n",
      "Layer 27 Accuracy: 1.0\n",
      "Layer 28 Accuracy: 1.0\n",
      "Layer 29 Accuracy: 1.0\n",
      "Layer 30 Accuracy: 1.0\n",
      "Layer 31 Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_pairs, val_pairs = train_test_split(prompt_pairs, test_size=0.5)\n",
    "\n",
    "for layer_idx in range(32): # Assuming 32 layers\n",
    "    X_train_correct, X_train_incorrect = [], []\n",
    "    \n",
    "    # Training Loop\n",
    "    for correct_prompt, incorrect_prompt in train_pairs:\n",
    "        model.get_logits(correct_prompt)\n",
    "        correct_activations = model.model.model.layers[layer_idx].intermediate_res.cpu().numpy()\n",
    "        correct_activations = np.array([act.flatten() for act in correct_activations])\n",
    "        model.get_logits(incorrect_prompt)\n",
    "        incorrect_activations = model.model.model.layers[layer_idx].intermediate_res.cpu().numpy()\n",
    "        incorrect_activations = np.array([act.flatten() for act in incorrect_activations])\n",
    "\n",
    "        # Accumulate the activations\n",
    "        X_train_correct.extend(correct_activations)\n",
    "        X_train_incorrect.extend(incorrect_activations)\n",
    "\n",
    "    X_train = np.vstack((X_train_correct, X_train_incorrect))\n",
    "    y_train = np.array([1] * len(X_train_correct) + [0] * len(X_train_incorrect))\n",
    "\n",
    "    # Train the Logistic Regression model\n",
    "    clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "    X_val_correct, X_val_incorrect = [], []\n",
    "    # Validation Loop\n",
    "    for correct_prompt, incorrect_prompt in val_pairs:\n",
    "        model.get_logits(correct_prompt)\n",
    "        correct_activations = model.model.model.layers[layer_idx].intermediate_res.cpu().numpy()\n",
    "        correct_activations = np.array([act.flatten() for act in correct_activations])\n",
    "        model.get_logits(incorrect_prompt)\n",
    "        incorrect_activations = model.model.model.layers[layer_idx].intermediate_res.cpu().numpy()\n",
    "        incorrect_activations = np.array([act.flatten() for act in incorrect_activations])\n",
    "\n",
    "        # Accumulate the activations\n",
    "        X_val_correct.extend(correct_activations)\n",
    "        X_val_incorrect.extend(incorrect_activations)\n",
    "\n",
    "    X_val = np.vstack((X_val_correct, X_val_incorrect))\n",
    "    y_val = np.array([1] * len(X_val_correct) + [0] * len(X_val_incorrect))\n",
    "\n",
    "\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    \n",
    "    print(f\"Layer {layer_idx} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Accuracy: 1.0\n",
      "Layer 1 Accuracy: 1.0\n",
      "Layer 2 Accuracy: 1.0\n",
      "Layer 3 Accuracy: 1.0\n",
      "Layer 4 Accuracy: 1.0\n",
      "Layer 5 Accuracy: 1.0\n",
      "Layer 6 Accuracy: 1.0\n",
      "Layer 7 Accuracy: 1.0\n",
      "Layer 8 Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_pairs, val_pairs = train_test_split(prompt_pairs, test_size=0.5)\n",
    "\n",
    "for layer_idx in range(32): # Assuming 32 layers\n",
    "    X_train_correct, X_train_incorrect = [], []\n",
    "    \n",
    "    # Training Loop\n",
    "    for correct_prompt, incorrect_prompt in train_pairs:\n",
    "        model.get_logits(correct_prompt)\n",
    "        correct_activations = model.model.model.layers[layer_idx].block_output.cpu().numpy()\n",
    "        correct_activations = np.array([act.flatten() for act in correct_activations])\n",
    "        model.get_logits(incorrect_prompt)\n",
    "        incorrect_activations = model.model.model.layers[layer_idx].block_output.cpu().numpy()\n",
    "        incorrect_activations = np.array([act.flatten() for act in incorrect_activations])\n",
    "\n",
    "        # Accumulate the activations\n",
    "        X_train_correct.extend(correct_activations)\n",
    "        X_train_incorrect.extend(incorrect_activations)\n",
    "\n",
    "    X_train = np.vstack((X_train_correct, X_train_incorrect))\n",
    "    y_train = np.array([1] * len(X_train_correct) + [0] * len(X_train_incorrect))\n",
    "\n",
    "    # Train the Logistic Regression model\n",
    "    clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "    X_val_correct, X_val_incorrect = [], []\n",
    "    # Validation Loop\n",
    "    for correct_prompt, incorrect_prompt in val_pairs:\n",
    "        model.get_logits(correct_prompt)\n",
    "        correct_activations = model.model.model.layers[layer_idx].intermediate_res.cpu().numpy()\n",
    "        correct_activations = np.array([act.flatten() for act in correct_activations])\n",
    "        model.get_logits(incorrect_prompt)\n",
    "        incorrect_activations = model.model.model.layers[layer_idx].intermediate_res.cpu().numpy()\n",
    "        incorrect_activations = np.array([act.flatten() for act in incorrect_activations])\n",
    "\n",
    "        # Accumulate the activations\n",
    "        X_val_correct.extend(correct_activations)\n",
    "        X_val_incorrect.extend(incorrect_activations)\n",
    "\n",
    "    X_val = np.vstack((X_val_correct, X_val_incorrect))\n",
    "    y_val = np.array([1] * len(X_val_correct) + [0] * len(X_val_incorrect))\n",
    "\n",
    "\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    \n",
    "    print(f\"Layer {layer_idx} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a correct answer. The Taj Mahal is in the country of [/INST]\n",
    "\"\"\"\n",
    "\n",
    "incorrect = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant who responds with one word.\n",
    "<</SYS>>\n",
    "\n",
    "Respond with a incorrect answer. The Taj Mahal is in the country of [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_logits(correct)  # Triggers the forward pass\n",
    "residual_activations = []\n",
    "block_activations = []\n",
    "for layer in model.model.model.layers:\n",
    "    residual_activations.append(layer.intermediate_res.cpu().numpy())\n",
    "    block_activations.append(layer.block_output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 51, 4096)\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(residual_activations[31].shape) # batch_size, sequence_length, hidden_dimension\n",
    "print(len(block_activations)) # 32 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_logits(incorrect)  # Triggers the forward pass\n",
    "i_residual_activations = []\n",
    "i_block_activations = []\n",
    "for layer in model.model.model.layers:\n",
    "    i_residual_activations.append(layer.intermediate_res.cpu().numpy())\n",
    "    i_block_activations.append(layer.block_output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 208896)\n",
      "(1, 208896)\n",
      "(2, 208896)\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "# Concatenate and reshape the activations\n",
    "X_train_correct = np.array([act.flatten() for act in residual_activations[31]])\n",
    "X_train_incorrect = np.array([act.flatten() for act in i_residual_activations[31]])\n",
    "print(X_train_correct.shape)\n",
    "print(X_train_incorrect.shape)\n",
    "# Combine the correct and incorrect activations\n",
    "X_train = np.vstack((X_train_correct, X_train_incorrect))\n",
    "print(X_train.shape)\n",
    "# Create labels (1 for correct, 0 for incorrect)\n",
    "y_train = np.array([1] * X_train_correct.shape[0] + [0] * X_train_incorrect.shape[0])\n",
    "print(y_train)\n",
    "clf = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = clf.predict(X_train)\n",
    "print(y_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_all()\n",
    "layer = 14\n",
    "model.get_logits('bananas')\n",
    "attn = model.get_attn_activations(layer)\n",
    "last_token_attn = attn[0][-1]\n",
    "model.set_add_attn_output(layer, 0.6*last_token_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pineapples are a delicious fruit 🍑\\nI love pineapples! ��������������������������'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_text('Pineapples are a delicious fruit ', max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vicuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
